# ğŸ™ï¸ Emotion Recognition from Voice using Machine Learning

## ğŸ˜ƒğŸ”Š Project Overview

This project is a Machine Learning-based system that detects **emotions from raw human voice recordings** â€” without relying on speech-to-text. By analyzing **acoustic features** such as pitch, MFCCs, tone, and spectrograms, the model classifies speech into emotional states like *happy, sad, angry, neutral*, and more.

> âš¡ Built to be **language-independent**, this system is ideal for real-time applications in **mental health monitoring**, **empathetic voice assistants**, and **customer service analytics**.

---

## ğŸš€ Key Features

- ğŸ§ **Audio-only Emotion Detection** (No transcription needed)
- ğŸ“Š Feature extraction: **MFCCs, Chroma, Spectral Contrast, Tonnetz**
- ğŸ§  Trained on multilingual emotional speech datasets
- ğŸŒ Language-agnostic design
- ğŸ“ˆ Supports real-time and batch predictions
- ğŸ§ª Includes test data and visualization scripts

---

## ğŸ§ª How It Works

### 1ï¸âƒ£ Feature Extraction
We extract acoustic features using `librosa`:
- MFCC (Mel Frequency Cepstral Coefficients)
- Chroma Frequencies
- Spectral Contrast
- Tonnetz
- Zero Crossing Rate, Energy, etc.

### 2ï¸âƒ£ Model Training
Models used:
- SVM (Support Vector Machine)
- MLP (Multi-Layer Perceptron)
- Optionally: CNN for spectrograms

### 3ï¸âƒ£ Emotion Prediction
Feed an audio sample to the model:
```bash
python src/predict.py --file data/test_sample.wav
```

## ğŸ› ï¸ Installation
### 1. âš™ï¸ Requirements

```bash
python pip install -r requirements.txt
```
### Or

### 2. ğŸ Create a Virtual Environment
```bash
python python -m venv venv
source venv/bin/activate # on macOS/Unix
venv\Scripts\activate # on Windows
pip install -r requirements.txt